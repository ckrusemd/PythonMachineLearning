---
title: "Chapter 3"
output: html_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 3: A Tour of Machine Learning Classifiers Using scikit-learn

```{python}

from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt

iris = datasets.load_iris()
X = iris.data[:, [2, 3]]
y = iris.target

X[1:10,]
np.unique(y)


```

## Explore iris data and training-test splits

```{python}

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=1, stratify=y)

X_train[1:10,]
y_train

len(y_train)
len(y_test)

np.bincount(y_train)
np.bincount(y_test)

```

## Standardize

Sample mean and sample standard deviation, then transformed.

```{python}

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

X_train[1:10,]
X_train_std[1:10,]

```


### Perceptron

```{python}
from sklearn.linear_model import Perceptron

ppn = Perceptron(eta0=0.1, random_state=1)
ppn.fit(X_train_std, y_train)

```


```{python}

## Test
y_pred = ppn.predict(X_test_std)

y_pred

y_test

y_pred==y_test

print("Correctly classified: %d" % (y_pred==y_test).sum())
print("Wrongly classified: %d" % (y_pred!=y_test).sum())


## Straight method
from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)

```

```{python}

X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

plt.clf()
plot_decision_regions(X=X_combined_std,y=y_combined,classifier=ppn)
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

```


## Logistic regression

### Odds, logit

Note that log refers to the natural logarithm,

```{python}

import math

p = 0.25

def odds(p):
  return( p/(1-p) )
def logit(p):
  return( math.log(p/(1-p)) )

odds(p)
logit(p)

```

Now, we are actually interested in predicting the probability that a certain example belongs to a particular class, which is the inverse form of the logit function.

It is also called the logistic sigmoid function, which is sometimes simply abbreviated to sigmoid function due to its characteristic S-shape:

```{python}

def sigmoid(z):
  return( 1.0 / (1.0+np.exp(-z)) )

z = np.arange(-7, 7, 0.1)
phi_z = sigmoid(z)

plt.clf()
plt.plot(z, phi_z)
plt.axvline(0.0, color='k')
plt.ylim(-0.1, 1.1)
plt.xlabel('z')
plt.ylabel('$\phi (z)$')
# y axis ticks and gridline
plt.yticks([0.0, 0.5, 1.0])
ax = plt.gca()
ax.yaxis.grid(True)
plt.tight_layout()
plt.show()

```

```{python}

class LogisticRegressionGD(object):
    def __init__(self, eta=0.05, n_iter=100, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state
    
    def fit(self, X, y):
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01,
                              size=1 + X.shape[1])
        self.cost_ = []
        print("Initial weights: ")
        print(self.w_)
        
        for i in range(self.n_iter):
            net_input = self.net_input(X)
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            
            # note that we compute the logistic `cost` now
            # instead of the sum of squared errors cost
            cost = (-y.dot(np.log(output)) -
                        ((1 - y).dot(np.log(1 - output))))
            self.cost_.append(cost)
            if (i%50==0):
              print("Iteration:" + str(i))
              print("- Weights:")
              print(self.w_)
              print("- Cost:")
              print(cost)
              print("\n")
        return self
        
    def net_input(self, X):
        return np.dot(X, self.w_[1:]) + self.w_[0]
        
    def activation(self, z):
        return 1. / (1. + np.exp(-np.clip(z, -250, 250)))
        
    def predict(self, X):
        return np.where(self.net_input(X) >= 0.0, 1, 0)


```

So, let's consider only Iris-setosa and Iris-versicolor flowers (classes 0 and 1) and check that our implementation of logistic regression works:

```{python}

X_train_01_subset = X_train[(y_train == 0) | (y_train == 1)]
y_train_01_subset = y_train[(y_train == 0) | (y_train == 1)]
lrgd = LogisticRegressionGD(eta=0.05,n_iter=1000,random_state=1)
lrgd.fit(X_train_01_subset,y_train_01_subset)

plt.clf()
plot_decision_regions(X=X_train_01_subset,y=y_train_01_subset,classifier=lrgd)
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()


```

### Sklearn Logistic Regression

Via `sklearn`

```{python}

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=100.0, random_state=1,solver='lbfgs', multi_class='ovr')
lr.fit(X_train_std, y_train)
plot_decision_regions(X_combined_std,y_combined,classifier=lr)
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()


```

### Predicting a single sample

```{python}

# Single
lr.predict_proba(X_test_std[:1,:])

lr.predict_proba(X_test_std[:1,:]).argmax(axis=1)

# Multiple
lr.predict_proba(X_test_std[:5,:])

lr.predict_proba(X_test_std[:5,:]).argmax(axis=1)

```


## Tackling overfitting via regularization

If a model suffers from `overfitting`, we also say that the model has a high variance, which can be caused by having too many parameters, leading to a model that is too complex given the underlying data

Similarly, our model can also suffer from `underfitting (high bias)`, which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data.


The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter (weight) values. The most common form of regularization is so-called L2 regularization (sometimes also called `L2 shrinkage` or `weight decay`), which can be written as follows:

## Maximum margin classification with support vector machines

```{python}

from sklearn.svm import SVC
svm = SVC(kernel='linear', C=1.0, random_state=1)
svm.fit(X_train_std, y_train)
plot_decision_regions(X_combined_std,y_combined,classifier=svm)
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

```

## Decision tree learning


```{python}
from sklearn.tree import DecisionTreeClassifier
tree_model = DecisionTreeClassifier(criterion='gini',max_depth=4,random_state=1)
tree_model.fit(X_train, y_train)
X_combined = np.vstack((X_train, X_test))
y_combined = np.hstack((y_train, y_test))

plt.clf()
plot_decision_regions(X_combined,y_combined,classifier=tree_model)
plt.xlabel('petal length [cm]')
plt.ylabel('petal width [cm]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

```


â†‘
```{python}
from sklearn import tree
tree.plot_tree(tree_model)
plt.show()

```

## Random forest

```{python}

from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(criterion='gini',n_estimators=25,random_state=1,n_jobs=2)
forest.fit(X_train, y_train)

plt.clf()
plot_decision_regions(X_combined, y_combined,classifier=forest)
plt.xlabel('petal length [cm]')
plt.ylabel('petal width [cm]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

```

### KNN

```{python}

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5, p=2,metric='minkowski')
knn.fit(X_train_std, y_train)

plt.clf()
plot_decision_regions(X_combined_std, y_combined,classifier=knn)
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

```

